# References

## Models
- **CodeBERT**  
  - Paper: [CodeBERT: A Pre-Trained Model for Programming and Natural Languages (Feng et al., EMNLP 2020)](https://arxiv.org/abs/2002.08155)  
  - HuggingFace Model Card: [microsoft/codebert-base](https://huggingface.co/microsoft/codebert-base)  
  - License: MIT  

- **CodeT5**  
  - Paper: [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation (Wang et al., EMNLP 2021)](https://arxiv.org/abs/2109.00859)  
  - HuggingFace Model Card: [Salesforce/codet5-base](https://huggingface.co/Salesforce/codet5-base)  
  - License: Apache-2.0  

- **StarCoder**  
  - Paper: [StarCoder: may the source be with you! (Li et al., BigCode 2023)](https://arxiv.org/abs/2305.06161)  
  - HuggingFace Model Card: [bigcode/starcoder](https://huggingface.co/bigcode/starcoder)  
  - License: OpenRAIL-M  

## Educational Background
- [Socratic Tutoring in Intelligent Education Systems (AIED 2015)](https://www.aaied.org/proceedings/2015/longpapers/182.pdf)  
- [Conceptual Change in Programming Education (ACM TOCE 2019)](https://dl.acm.org/doi/10.1145/3344429)  

## Tooling
- [Scikit-learn: Model Evaluation and Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)  
