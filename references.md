# References

**Models**
- CodeBERT (Feng et al., EMNLP 2020). Model card: https://huggingface.co/microsoft/codebert-base  
- CodeT5 (Wang et al., EMNLP 2021). Model card: https://huggingface.co/Salesforce/codet5-base  
- StarCoder (Li et al., BigCode 2023). Model card: https://huggingface.co/bigcode/starcoder  

**Background**
- Feng et al., “CodeBERT: A Pre-Trained Model for Programming and Natural Languages.” EMNLP 2020.  
- Wang et al., “CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.” EMNLP 2021.  
- Li et al., “StarCoder: may the source be with you!” BigCode, 2023.  
- Socratic Tutoring in Intelligent Education Systems — AIED 2015.  
- Conceptual Change in Programming Education — ACM TOCE 2019.

**Licensing**
- CodeBERT: MIT  
- CodeT5: Apache-2.0  
- StarCoder: OpenRAIL-M  
